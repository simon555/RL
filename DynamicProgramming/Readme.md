# Dynamic Programming

We start our RL journey by implementing the basic RL algorithms, using Dynamic Programming

## Policy Evaluation
Below, we track the evolution of the differences between the updates of the Value function that is computed. Through the training, the difference converges towards 0 meaning that the value function converged towards the expected value.

<<<<<<< HEAD
![alt text](https://github.com/simon555/RL/blob/master/DP/PolicyEvaluation/PolicyEvaluation.png)

As final output, we get the value function on the gridworld : 

![alt ](https://github.com/simon555/RL/blob/master/DP/PolicyEvaluation/EvolutionPolicyEvaluation.png)
=======


![alt text](https://github.com/simon555/RL/blob/master/DynamicProgramming/PolicyEvaluation.png)

As final output, we get the value function on the gridworld : 

![alt text](https://github.com/simon555/RL/blob/master/DynamicProgramming/EvolutionPolicyEvaluation.png)
>>>>>>> e1cd6ac799c61ada2916576f56c859579da141df
