# Dynamic Programming

We start our RL journey by implementing the basic RL algorithms, using Dynamic Programming

## Policy Evaluation
Below, we track the evolution of the differences between the updates of the Value function that is computed. Through the training, the difference converges towards 0 meaning that the value function converged towards the expected value.



![alt text](https://github.com/simon555/RL/blob/master/DynamicProgramming/PolicyEvaluation.png)

As final output, we get the value function on the gridworld : 

![alt text](https://github.com/simon555/RL/blob/master/DynamicProgramming/EvolutionPolicyEvaluation.png)
